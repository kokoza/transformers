{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Metrics\n",
    "\n",
    "We've put together our first Q&A model, and explored some of the metrics we can use to measure Q&A performance. In this notebook we're going to merge both of these and measure our Q&A model performance on the SQuAD 2.0 validation set as a whole.\n",
    "\n",
    "First, we load our SQuAD validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 16:57:55.944208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 16:57:56.043709: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 16:57:56.364982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 16:57:56.365046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 16:57:56.365050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from transformers import pipeline\n",
    "\n",
    "with open('../../data/squad/dev.json', 'r') as f:\n",
    "    squad = json.load(f)\n",
    "\n",
    "modelname = 'deepset/bert-base-cased-squad2'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(modelname)\n",
    "model = BertForQuestionAnswering.from_pretrained(modelname)\n",
    "\n",
    "\n",
    "qa = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# intialize a list for answers\n",
    "answers = []\n",
    "\n",
    "for pair in squad[:5]:\n",
    "    # pass in our question and context to return an answer\n",
    "    ans = qa({\n",
    "        'question': pair['question'],\n",
    "        'context': pair['context']\n",
    "    })\n",
    "    # append predicted answer and real to answers list\n",
    "    answers.append({\n",
    "        'predicted': ans['answer'],\n",
    "        'true': pair['answer']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's setup the QA pipeline again using the `deepset/bert-base-cased-squad2` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we build a list of predicted answers `model_out` and true answers `reference` and calculate the ROUGE score based on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_out = []\n",
    "reference = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for pair in tqdm(squad[:100], leave=True):\n",
    "    ans = qa({\n",
    "        'question': pair['question'],\n",
    "        'context': pair['context']\n",
    "    })\n",
    "    # append the prediction and reference to the respective lists\n",
    "    model_out.append(ans['answer'])\n",
    "    reference.append(pair['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This make take some time to process. The processing speed of our models will improve as we begin using more efficient implementations over the next few sections.\n",
    "\n",
    "Once that has finished processing, we can calculate our ROUGE scores just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.4621428571428571,\n",
       "  'p': 0.4565714285714286,\n",
       "  'f': 0.4363357728149752},\n",
       " 'rouge-2': {'r': 0.2314404761904762,\n",
       "  'p': 0.24515151515151515,\n",
       "  'f': 0.2218395480658946},\n",
       " 'rouge-l': {'r': 0.4621428571428571,\n",
       "  'p': 0.4565714285714286,\n",
       "  'f': 0.4363357728149752}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem to be scoring as high as we would expect, if we print some of the results we can see why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate individual scores\n",
    "scores = rouge.get_scores(model_out, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollo,  |  Rollo  |  0.0\n"
     ]
    }
   ],
   "source": [
    "print(model_out[4], ' | ', reference[4], ' | ', scores[4]['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Norseman, Viking\".  |  Viking  |  0.0\n"
     ]
    }
   ],
   "source": [
    "print(model_out[22], ' | ', reference[22], ' | ', scores[22]['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the punctuation differences are causing our ROUGE score to view these words as not matching. To fix this, we'll import `re` and remove any characters that are not spaces, letters, or numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean = re.compile('(?i)[^0-9a-z ]')\n",
    "\n",
    "# apply this to both lists\n",
    "model_out = [clean.sub('', text) for text in model_out]\n",
    "reference = [clean.sub('', text) for text in reference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate individual scores\n",
    "scores = rouge.get_scores(model_out, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollo  |  Rollo  |  0.999999995\n"
     ]
    }
   ],
   "source": [
    "print(model_out[4], ' | ', reference[4], ' | ', scores[4]['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norseman Viking  |  Viking  |  0.6666666622222223\n"
     ]
    }
   ],
   "source": [
    "print(model_out[22], ' | ', reference[22], ' | ', scores[22]['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are looking better now, let's calculate the average again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.6263095238095236,\n",
       "  'p': 0.6139090909090908,\n",
       "  'f': 0.5770818038114618},\n",
       " 'rouge-2': {'r': 0.2764404761904762,\n",
       "  'p': 0.2815800865800866,\n",
       "  'f': 0.25933954789401953},\n",
       " 'rouge-l': {'r': 0.6263095238095236,\n",
       "  'p': 0.6139090909090908,\n",
       "  'f': 0.5770818038114618}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are seeing much more realistic scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recall, Precision, F1 and ROUGE-L (Longest Common Subsequence, LCS)\n",
    "\n",
    "## Recall\n",
    "The recall counts the number of overlapping n-grams found in both the model output and reference — then divides this number by the total number of n-grams in the reference. It looks like this:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_recall.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "This is great for ensuring our model is **capturing all of the information** contained in the reference — but this isn’t so great at ensuring our model isn’t just pushing out a huge number of words to game the recall score:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_gaming_recall.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Precision\n",
    "\n",
    "To avoid this we use the precision metric — which is calculated in almost the exact same way, but rather than dividing by the reference n-gram count, we divide by the model n-gram count.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_precision_calc.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "So if we apply this to our previous example, we get a precision score of just 43%:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_precision_fixes_recall.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## F1-Score\n",
    "\n",
    "Now that we both the recall and precision values, we can use them to calculate our ROUGE F1 score like so:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_f1_calc.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Let's apply that again to our previous example:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_f1.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "That gives us a reliable measure of our model performance that relies not only on the model capturing as many words as possible (recall) but doing so without outputting irrelevant words (precision).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-L\n",
    "\n",
    "ROUGE-L measures the **Longest Common Subsequence (LCS)** between our model output and reference. All this means is that we count the longest sequence of tokens that is shared between both:\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_l.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "The idea here is that a longer shared sequence would indicate more similarity between the two sequences. We can apply our recall and precision calculations just like before — but this time we replace the match with LCS.\n",
    "\n",
    "First we calculate the LCS recall \n",
    "\n",
    "### LCS Recall = number of longest common subsequenc / number of tokens in the reference text\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_l_recall.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Precision is the same, we just switch our total n-gram count from the reference to the model:\n",
    "\n",
    "### LCS Precision = number of longest common subsequenc / number of tokens in the predicted text\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_l_precision.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "And finally, we calculate the F1 score just like we did before:\n",
    "\n",
    "### LCS F1-Score = 2 * (LCS Recall * LCS Precision) / (LCS Recall + LCS Precision)\n",
    "\n",
    "<div>\n",
    "<img src=\"../../assets/images/rouge_l_f1.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### ROUGE-S\n",
    "\n",
    "# The final ROUGE metric we will look at is the ROUGE-S — or skip-gram concurrence metric.\n",
    "\n",
    "# Now, this metric is much less popular than ROUGE-N and ROUGE-L covered already — but it’s worth being aware of what it does.\n",
    "\n",
    "# Using the skip-gram metric allows us to search for consecutive words from the reference text, that appear in the model output but are separated by one-or-more other words.\n",
    "\n",
    "# So, if we took the bigram “the fox”, our original ROUGE-2 metric would only match this if this exact sequence was found in the model output. If the model instead outputs “the brown fox” — no match would be found.\n",
    "\n",
    "# ROUGE-S allows us to add a degree of leniency to our n-gram matching. For our bigram example we could match by using a skip-bigram measure:\n",
    "\n",
    "# ![ROUGE-S recall](../../assets/images/rouge_s_recall.png)\n",
    "\n",
    "# The same logic applies to our precision metric too:\n",
    "\n",
    "# ![ROUGE-S precision](../../assets/images/rouge_s_precision.png)\n",
    "\n",
    "# After calculating our recall and precision, we can calculate the F1 score too just as we did before.\n",
    "\n",
    "# ### Cons\n",
    "\n",
    "# ROUGE is a great evaluation metric but comes with some drawbacks. In-particular, ROUGE does not cater for different words that have the same meaning — as it measures syntactical matches rather than semantics.\n",
    "\n",
    "# So, if we had two sequences that had the same meaning — but used different words to express that meaning — they could be assigned a low ROUGE score.\n",
    "\n",
    "# This can be offset slightly by using several references and taking the average score, but this will not solve the problem entirely.\n",
    "\n",
    "# Nonetheless, it’s a good metric which is very popular for assessing the performance of several NLP tasks, including machine translation, automatic summarization, and *for us*, question-and-answering.\n",
    "\n",
    "# ## In Python\n",
    "\n",
    "# We've worked through the theory of the ROUGE metrics and how they work. Fortunately, implementing these metrics in Python is incredibly easy thanks to the Python rouge library.\n",
    "\n",
    "# We can install the library through pip:\n",
    "\n",
    "# ```\n",
    "# pip install rouge\n",
    "# ```\n",
    "\n",
    "# And scoring our model output against a reference is as easy as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
