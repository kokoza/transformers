{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dense Vectors Using Transformers\n",
    "\n",
    "We will be using the [`sentence-transformers/stsb-distilbert-base`](https://huggingface.co/sentence-transformers/stsb-distilbert-base) model to build our dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize our model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776cacc56c3c44d1901cd22b5ffffa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d38bbc3d434401a8b5d53fc265a8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596dd93a7682431985873f6bf6a9c822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3771aa7321184830a1a2f2857a279cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c49a154139400ea978c03960fd0f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976259569cce454292f0ef609c3fb589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e90a1a6b42a424ab508202d75e7774f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tokenize a sentence just as we have been doing before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello world what a time to be alive!\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(text, max_length=128,\n",
    "                               truncation=True, padding='max_length',\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process these tokens through our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.0681e-01, -7.8805e-02,  1.7431e+00,  ..., -2.5349e-02,\n",
       "          -1.1080e-01,  4.8311e-02],\n",
       "         [ 7.1302e-01,  1.0437e-01,  1.8346e+00,  ...,  1.1343e-01,\n",
       "          -7.5564e-02,  1.2668e-01],\n",
       "         [ 8.1722e-01,  1.1321e-01,  1.5408e+00,  ..., -3.8067e-01,\n",
       "           8.7477e-02, -1.9020e-01],\n",
       "         ...,\n",
       "         [ 5.4669e-01,  1.7181e-01,  1.1392e+00,  ...,  3.8548e-02,\n",
       "          -1.5396e-01,  2.3015e-01],\n",
       "         [ 3.4457e-01,  1.3151e-01,  1.1324e+00,  ..., -1.4211e-03,\n",
       "          -1.7517e-01,  1.5220e-01],\n",
       "         [ 3.2320e-01,  3.3350e-03,  1.1888e+00,  ...,  1.6736e-02,\n",
       "          -2.0864e-01,  8.9316e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Hidden State Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense vector representations of our `text` are contained within the `outputs` **'last_hidden_state'** tensor, which we access like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.0681e-01, -7.8805e-02,  1.7431e+00,  ..., -2.5349e-02,\n",
       "          -1.1080e-01,  4.8311e-02],\n",
       "         [ 7.1302e-01,  1.0437e-01,  1.8346e+00,  ...,  1.1343e-01,\n",
       "          -7.5564e-02,  1.2668e-01],\n",
       "         [ 8.1722e-01,  1.1321e-01,  1.5408e+00,  ..., -3.8067e-01,\n",
       "           8.7477e-02, -1.9020e-01],\n",
       "         ...,\n",
       "         [ 5.4669e-01,  1.7181e-01,  1.1392e+00,  ...,  3.8548e-02,\n",
       "          -1.5396e-01,  2.3015e-01],\n",
       "         [ 3.4457e-01,  1.3151e-01,  1.1324e+00,  ..., -1.4211e-03,\n",
       "          -1.7517e-01,  1.5220e-01],\n",
       "         [ 3.2320e-01,  3.3350e-03,  1.1888e+00,  ...,  1.6736e-02,\n",
       "          -2.0864e-01,  8.9316e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors `embeddings`, we need to perform a *mean pooling* operation on them to create a single vector encoding (the **sentence embedding**). To do this mean pooling operation we will need to multiply each value in our `embeddings` tensor by it's respective `attention_mask` value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our `attention_mask` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add an additional dimension to tensor\n",
    "attention_mask.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand to match the size of our embeddings\n",
    "attention_mask.unsqueeze(-1).expand(embeddings.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's *attention_mask* status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3068, -0.0788,  1.7431,  ..., -0.0253, -0.1108,  0.0483],\n",
       "         [ 0.7130,  0.1044,  1.8346,  ...,  0.1134, -0.0756,  0.1267],\n",
       "         [ 0.8172,  0.1132,  1.5408,  ..., -0.3807,  0.0875, -0.1902],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.0681e-01, -7.8805e-02,  1.7431e+00,  ..., -2.5349e-02,\n",
       "          -1.1080e-01,  4.8311e-02],\n",
       "         [ 7.1302e-01,  1.0437e-01,  1.8346e+00,  ...,  1.1343e-01,\n",
       "          -7.5564e-02,  1.2668e-01],\n",
       "         [ 8.1722e-01,  1.1321e-01,  1.5408e+00,  ..., -3.8067e-01,\n",
       "           8.7477e-02, -1.9020e-01],\n",
       "         ...,\n",
       "         [ 5.4669e-01,  1.7181e-01,  1.1392e+00,  ...,  3.8548e-02,\n",
       "          -1.5396e-01,  2.3015e-01],\n",
       "         [ 3.4457e-01,  1.3151e-01,  1.1324e+00,  ..., -1.4211e-03,\n",
       "          -1.7517e-01,  1.5220e-01],\n",
       "         [ 3.2320e-01,  3.3350e-03,  1.1888e+00,  ...,  1.6736e-02,\n",
       "          -2.0864e-01,  8.9316e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the embeddings toward the end have been set to zero by our attention mask/padding\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the remained of the embeddings along axis `1`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take mean of all embeddings to get a 1x768 vector\n",
    "\n",
    "#### First take the sum along the axis with length = 128 (axis 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then sum the number of values that must be given attention in each position of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations `summed` divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.4371e-01, -1.6713e-01,  1.6938e+00,  1.0773e-01,  3.9651e-01,\n",
       "         -1.0034e-01, -4.7002e-02,  7.9962e-01,  9.9557e-02, -9.4254e-01,\n",
       "         -2.3453e-01, -2.0168e-02,  1.9673e-01,  3.9952e-01,  1.1500e-01,\n",
       "          3.4273e-01, -6.4042e-01, -5.6996e-01,  2.7601e-01, -6.6139e-01,\n",
       "         -7.0416e-01, -2.9305e-01, -1.5104e-01, -2.2795e-01,  9.0203e-01,\n",
       "         -5.3868e-01, -1.0533e-01, -6.9518e-01, -3.7914e-01,  2.6069e-01,\n",
       "         -8.8313e-01, -1.1394e-01,  1.0662e+00, -5.9388e-01, -4.5752e-01,\n",
       "          1.4677e+00, -6.4333e-01, -1.7422e-01,  1.2722e-01, -4.2277e-01,\n",
       "          6.6559e-01, -1.9796e-01,  1.1205e+00,  2.3012e-01, -1.3350e+00,\n",
       "          4.1537e-01, -5.1203e-01,  4.1024e-01,  2.7452e-01, -8.5214e-01,\n",
       "          1.2710e-01, -8.0763e-01, -6.3740e-01, -3.1229e-01, -4.9854e-01,\n",
       "          3.2761e-01,  3.9804e-01, -4.4230e-01, -8.0381e-02,  4.9029e-01,\n",
       "          6.1305e-01, -5.9101e-01,  5.5611e-01,  1.7230e-01, -1.0206e+00,\n",
       "          1.7572e-01,  1.0949e+00, -3.6538e-01,  1.7774e-01,  4.3270e-01,\n",
       "          1.0963e+00, -8.3898e-02, -2.6611e-01,  4.0809e-01, -1.3963e+00,\n",
       "         -8.0904e-01,  1.9847e-01,  3.2139e-01,  4.0050e-01,  9.5752e-01,\n",
       "          9.1508e-01, -5.3994e-01,  1.0592e+00,  9.7352e-01, -6.0041e-01,\n",
       "          2.1563e-01,  1.3225e-01,  8.6220e-02, -2.0152e+00,  6.4119e-01,\n",
       "         -6.9878e-02, -7.2408e-01,  1.1759e+00,  1.8488e-01,  3.4277e-01,\n",
       "          6.5096e-01, -3.9148e-02, -6.0620e-01, -5.0671e-01, -3.2001e-01,\n",
       "          2.9667e-02, -2.2884e-01, -2.4417e-01,  5.7870e-02,  3.7981e-01,\n",
       "          4.9694e-01, -6.0655e-02, -1.2346e+00,  6.1337e-01,  9.5491e-02,\n",
       "          8.8232e-01,  4.7992e-02,  7.6416e-02,  6.4590e-01,  5.0325e-01,\n",
       "         -8.7077e-02, -1.4783e+00,  2.9612e-01,  4.6578e-02,  1.7338e-01,\n",
       "         -4.9755e-01,  6.4269e-01, -1.5406e-01, -5.6126e-01,  2.0604e-01,\n",
       "         -5.6562e-01, -8.1897e-02,  1.3078e-01, -5.5340e-01, -6.7645e-02,\n",
       "         -1.3948e-01,  7.0307e-01,  4.4462e-01, -1.0322e+00,  8.4083e-02,\n",
       "          8.6769e-01, -2.0125e-01, -7.3381e-01, -1.0247e+00, -2.0466e-01,\n",
       "         -6.3981e-01,  3.8276e-01,  3.9684e-01,  4.6901e-01,  1.4168e-01,\n",
       "         -5.7647e-01,  2.1898e-01, -2.2738e-01, -8.3408e-03, -2.4746e-01,\n",
       "         -9.9057e-01, -3.0005e-01, -8.8709e-01, -9.2324e-01, -4.6641e-01,\n",
       "         -3.9903e-01, -1.1783e+00,  5.3981e-01,  2.1995e-01,  4.0774e-01,\n",
       "         -3.7131e-01,  1.2338e+00, -1.9070e-01,  2.0049e-01, -1.1310e-01,\n",
       "         -1.5072e-01,  1.2708e+00,  1.0951e+00,  1.4560e-01,  3.6235e-01,\n",
       "          6.5676e-01, -3.4773e-01,  1.7364e-01,  1.1239e+00, -6.8744e-01,\n",
       "         -2.6254e-01,  1.7511e+00,  6.2810e-01,  4.7067e-01, -4.5984e-01,\n",
       "          5.2457e-01, -4.5056e-01,  5.5844e-01, -4.0008e-02,  4.4211e-02,\n",
       "         -1.2678e+00,  2.3521e-01, -2.9582e-01,  9.0942e-01,  1.8530e-01,\n",
       "          1.5625e-01, -1.9385e-01,  1.1832e-01, -1.4211e+00, -7.4084e-01,\n",
       "         -1.2756e-01,  1.9533e-01,  8.3617e-01, -1.9950e-01, -2.5804e-01,\n",
       "          5.3352e-01,  6.2743e-03,  9.5528e-01, -3.0540e-01,  1.2990e+00,\n",
       "          1.0194e+00,  1.3714e-01, -1.2520e-01, -3.0013e-01, -6.1435e-01,\n",
       "         -3.1806e-01,  1.1757e+00,  6.5084e-01, -1.7192e-01, -7.3129e-02,\n",
       "         -5.0328e-01,  7.4073e-01, -3.1717e-01, -1.0501e+00, -2.4812e-01,\n",
       "          5.9124e-01,  4.1397e-01,  1.5154e-02, -2.5077e-01,  3.3859e-01,\n",
       "          6.9724e-01,  7.8416e-01, -1.0414e+00, -1.2615e-01, -6.9690e-01,\n",
       "          1.7303e-01, -3.2313e-01,  2.3371e-02,  1.4733e+00,  6.4430e-01,\n",
       "          7.0171e-01,  1.0446e-01, -7.5574e-02,  5.6683e-01,  1.6810e-01,\n",
       "          1.9664e-01,  7.8176e-01,  9.6536e-02, -3.0505e-01, -1.1925e+00,\n",
       "         -6.0975e-01,  1.0143e+00,  1.1556e-01, -1.1024e+00,  3.5646e-01,\n",
       "         -2.8990e-01, -8.0156e-01, -4.2613e-01,  2.9002e-01, -2.7425e-01,\n",
       "         -3.9380e-01, -1.6371e+00, -3.6250e-01, -2.7436e-01, -4.3914e-01,\n",
       "          3.5285e-01, -7.9173e-01, -9.2452e-02, -1.9319e-01, -8.5891e-01,\n",
       "          1.9185e-02, -3.0863e-01, -4.3679e-01, -1.1833e-01,  3.6784e-01,\n",
       "         -5.3335e-01,  7.9402e-01, -5.2792e-02,  5.6997e-01,  7.6599e-01,\n",
       "         -2.6063e-01,  8.5726e-01,  1.2974e-01, -1.7000e-01,  2.4458e-01,\n",
       "         -2.8240e-01,  1.9680e-01, -2.4120e+00, -7.4284e-01, -6.5133e-02,\n",
       "         -2.5929e-01,  7.3756e-01, -4.7114e-01, -3.0464e-01, -4.5057e-01,\n",
       "         -4.4889e-01, -4.7355e-01,  2.5889e-01, -1.3260e+00,  1.5851e+00,\n",
       "         -3.8618e-01,  1.5764e+00, -9.4789e-01, -9.2309e-01,  4.6737e-01,\n",
       "         -4.8595e-01, -2.4293e-02, -5.5705e-01, -3.1455e-01, -1.1338e-01,\n",
       "         -1.6174e-01,  8.3162e-01, -5.6529e-01, -9.5904e-02, -1.1245e-01,\n",
       "         -1.0311e+00, -1.4277e-01, -7.3664e-01,  1.4765e-01, -2.2971e-01,\n",
       "          3.9673e-01, -1.0753e+00,  6.6455e-01,  3.1294e-01, -5.6631e-02,\n",
       "         -3.0951e-01,  2.2493e-01, -1.4850e-01,  2.2910e-01,  2.8938e-01,\n",
       "          6.7629e-01, -9.0940e-01,  4.4851e-01, -3.3832e-01, -3.3420e-01,\n",
       "          4.6286e-01, -3.9156e-01,  6.4487e-01,  3.9093e-01, -1.6772e-02,\n",
       "          9.6232e-01, -1.1312e+00, -4.0280e-01,  1.8588e-01,  2.6001e-01,\n",
       "          9.2113e-01, -2.7153e-01,  8.0074e-01,  2.8548e-01,  1.1330e+00,\n",
       "         -3.1117e-01, -4.8690e-01, -1.0499e+00,  9.8599e-03, -2.8706e-01,\n",
       "         -3.9961e-01, -8.8232e-01,  3.5943e-01, -7.9205e-01, -6.9823e-01,\n",
       "          6.5981e-01,  5.9179e-01, -3.4009e-01,  3.5631e-01, -9.0518e-02,\n",
       "          5.4404e-02,  5.2091e-01, -1.1772e-01, -2.1404e-02,  2.3441e-02,\n",
       "          1.0091e-01,  5.8143e-01, -9.0742e-01,  1.0502e+00,  7.0596e-02,\n",
       "          4.2021e-01, -4.0573e-01,  3.3023e-02,  5.4977e-01, -2.1155e-01,\n",
       "         -2.8467e-01, -2.1110e-01, -6.6788e-01,  3.9122e-01,  2.0167e-01,\n",
       "          3.2140e-01, -5.3764e-01, -6.4544e-02, -4.9503e-01,  1.2885e-01,\n",
       "          2.2294e-01, -1.6210e-01, -5.9338e-01,  8.6498e-01, -8.4387e-01,\n",
       "         -1.7239e-01,  5.8500e-01,  1.3635e+00, -1.5760e-01, -1.6470e-01,\n",
       "         -1.9212e+00,  4.5632e-01,  6.5662e-01,  3.0252e-01, -8.3668e-02,\n",
       "          1.1162e-01,  5.1380e-01,  7.7562e-01, -1.0398e+00,  1.4494e-01,\n",
       "          4.0355e-01,  2.7488e-01, -1.0001e-01, -4.9586e-01,  3.2831e-01,\n",
       "         -1.6557e-01,  2.4734e-02,  9.4796e-01,  6.5025e-01, -1.1657e+00,\n",
       "          9.7131e-01,  5.8161e-01, -1.2391e+00,  1.8583e-01,  9.6496e-01,\n",
       "         -6.7188e-01, -1.7395e-01, -4.7076e-01, -6.9996e-01, -3.6292e-02,\n",
       "          3.2326e-01,  6.5279e-01,  1.5857e-01, -3.8178e-01, -7.6021e-01,\n",
       "         -1.9283e-02,  6.0273e-01, -8.0231e-01,  3.0534e-01,  7.2340e-01,\n",
       "         -4.5853e-01,  8.5664e-01, -1.2392e+00, -2.1578e-01,  1.8629e-01,\n",
       "          5.8645e-01,  1.1212e-01, -8.2547e-01,  4.9638e-01,  1.1431e-01,\n",
       "         -6.6946e-01,  1.9746e-02,  6.7667e-01, -8.0185e-02, -8.2179e-02,\n",
       "          5.1119e-01, -4.2190e-01, -2.7063e-01,  6.8726e-01,  4.8040e-01,\n",
       "          4.6528e-01, -8.6009e-01,  4.8235e-01,  4.8802e-01, -1.9158e-01,\n",
       "         -4.9732e-01, -1.7302e-01,  7.3646e-02, -3.0176e-01,  4.4385e-01,\n",
       "         -1.6570e-01, -2.4044e-01, -3.1199e-01,  5.5935e-01,  8.4654e-02,\n",
       "          6.3183e-01, -3.6332e-01, -4.0035e-03,  5.4901e-02,  8.8018e-01,\n",
       "          8.0402e-01, -1.5738e+00,  7.6655e-01,  4.5431e-01,  1.3196e+00,\n",
       "         -2.2543e-01, -6.7367e-02, -5.6315e-01, -7.8337e-01, -5.9928e-01,\n",
       "          9.9388e-01,  4.2677e-01,  1.5565e-01,  1.2991e+00, -1.3823e-01,\n",
       "         -4.7330e-01, -9.6147e-01,  3.7844e-02, -1.2626e+00, -6.3209e-02,\n",
       "         -6.2502e-01, -5.9404e-01, -3.0698e-01, -4.3991e-02,  2.5447e-01,\n",
       "          5.6305e-02,  2.9622e-02, -1.4003e-01, -9.0930e-01,  3.9465e-01,\n",
       "          5.1400e-01,  3.4101e-01,  5.8502e-01,  8.8188e-03,  6.4684e-01,\n",
       "         -1.0679e+00,  1.9261e-01,  6.7560e-01, -1.2556e-01, -8.9373e-01,\n",
       "         -1.7192e-01, -6.2001e-02,  1.9043e-02, -1.3476e-01, -6.1177e-01,\n",
       "          2.7916e-01,  8.0688e-02, -2.0764e-02, -6.6006e-02, -9.8538e-01,\n",
       "          1.0077e+00,  1.2251e+00, -6.9004e-01,  3.8142e-01, -3.4579e-02,\n",
       "         -2.2671e-03,  1.3628e-01, -9.6970e-02, -2.3826e-01, -4.3809e-01,\n",
       "          1.2707e+00, -1.2643e+00, -1.2259e+00,  1.3252e-03,  1.6211e-01,\n",
       "          6.6284e-01, -1.2526e-01, -8.7850e-01, -3.7673e-01, -7.2941e-01,\n",
       "         -1.9985e-01, -3.4584e-01, -2.3764e-01,  2.0929e-01, -8.5265e-01,\n",
       "         -8.6959e-01,  7.3296e-02,  4.8670e-01, -2.1067e-01, -1.0492e+00,\n",
       "          2.7279e-01, -2.7046e-01,  4.3164e-01,  2.1435e-01,  1.9497e-01,\n",
       "         -1.8009e-01, -2.6097e-01, -1.0510e+00, -1.4399e-01, -4.4938e-01,\n",
       "          3.4237e-01, -4.2369e-01, -3.4243e-01,  4.7377e-01,  6.0406e-01,\n",
       "         -2.9574e-01, -5.2437e-01,  5.0023e-01,  7.5045e-01, -1.0506e+00,\n",
       "         -3.8015e-01,  4.2542e-01, -1.3104e-01, -1.1576e+00,  3.5461e-02,\n",
       "         -1.2123e+00,  8.1284e-01,  7.4158e-01, -6.1665e-02, -1.7858e-01,\n",
       "         -2.7233e-01,  1.6723e-01, -7.6189e-01, -4.6897e-02,  2.6138e-01,\n",
       "          2.2514e-01, -5.0467e-01, -7.6334e-02, -5.6870e-01, -5.9955e-01,\n",
       "         -7.1071e-02,  2.5748e-01,  1.3806e+00,  4.7266e-01,  7.3659e-01,\n",
       "         -1.2811e-01, -5.8549e-01, -6.6802e-01,  6.6773e-01, -2.9148e-01,\n",
       "          7.1404e-01, -2.7279e-01, -8.4168e-01,  2.3216e-01,  6.4598e-01,\n",
       "          3.3574e-01, -1.5344e+00, -1.7103e-01, -5.6746e-01, -2.4354e-01,\n",
       "         -8.5056e-01, -8.5887e-01, -2.2118e-01,  6.2552e-01,  2.9925e-01,\n",
       "          1.8296e-01,  6.1234e-01, -7.4984e-01, -3.3012e-01,  3.4227e-01,\n",
       "         -8.3695e-01, -3.7775e-01,  3.6594e-01,  2.3072e-02, -5.3120e-01,\n",
       "          9.7278e-01,  6.3312e-02,  9.7519e-02,  2.1973e-02,  1.7400e-01,\n",
       "          6.5451e-01, -5.9943e-01,  6.2152e-01,  8.3180e-01,  2.4243e-01,\n",
       "         -7.0655e-01,  2.0125e-01,  7.0657e-01, -2.8193e-01, -3.2747e-01,\n",
       "          4.0119e-01,  1.6697e-01,  1.2031e+00,  1.1114e+00,  6.7388e-01,\n",
       "          7.6634e-01, -2.3141e-01, -3.0810e-01, -3.1501e-02, -2.4703e-01,\n",
       "         -1.0457e-01, -2.6844e-02, -4.2274e-01,  1.2233e-01, -5.1854e-01,\n",
       "          3.2959e-01, -1.5013e-01,  3.2573e-01, -1.3740e-01,  3.7476e-01,\n",
       "         -3.8138e-01,  1.1587e-01,  5.0823e-01, -5.0498e-01, -1.6156e-01,\n",
       "          9.2563e-01, -4.4763e-01,  3.5749e-01, -1.3937e+00, -4.6959e-01,\n",
       "         -6.3569e-02,  4.8912e-01,  4.4158e-01, -3.9566e-01, -4.0239e-01,\n",
       "          7.6553e-01, -4.5417e-01,  6.0901e-01, -5.0284e-02, -1.1499e+00,\n",
       "          1.5333e-01, -1.3780e-01, -9.2101e-01, -3.8819e-01,  2.6015e-01,\n",
       "          4.7425e-01,  5.7948e-01, -5.1846e-01, -9.3090e-01,  1.6527e-01,\n",
       "          5.8723e-01,  3.8862e-01, -4.6652e-01,  7.7639e-01, -2.4311e-01,\n",
       "          6.1275e-02,  7.3001e-01, -4.7051e-01, -7.6680e-01, -2.5298e-01,\n",
       "         -8.6387e-01, -7.1005e-01,  6.4419e-01,  6.5554e-01,  3.1619e-01,\n",
       "          4.3090e-01,  1.9270e-01,  4.8640e-01,  3.2848e-01, -1.4970e-01,\n",
       "          1.0152e+00,  5.7544e-01,  6.1238e-01,  5.0055e-01, -3.4704e-01,\n",
       "         -8.8650e-01,  5.2076e-01, -5.7141e-01,  3.0447e-01, -6.1227e-01,\n",
       "          6.7365e-02,  1.1126e-01, -6.1991e-01, -7.5090e-01, -7.0721e-01,\n",
       "          4.9020e-02, -4.2850e-02, -7.3886e-01,  5.8957e-02,  2.0284e-01,\n",
       "          1.8277e-01, -3.4661e-01, -4.1227e-01, -3.4070e-01,  6.9573e-01,\n",
       "         -1.4571e-01, -7.2461e-01, -8.1861e-01, -4.3171e-01, -2.9195e-01,\n",
       "         -1.0024e+00, -1.7059e-01,  1.2326e-01,  1.5297e-01,  2.3317e-01,\n",
       "          1.3112e+00, -5.6125e-01, -8.0447e-02,  6.6030e-02,  6.3616e-01,\n",
       "          1.0764e+00, -3.7552e-01, -1.9723e-01, -1.2639e-01, -9.7839e-01,\n",
       "          7.1431e-01, -8.5014e-01,  5.3043e-02, -2.2865e-01, -7.5399e-02,\n",
       "          8.2278e-02, -1.7042e-01, -8.3766e-02]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed / counts\n",
    "\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we calculate our dense similarity vector / sentence vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
