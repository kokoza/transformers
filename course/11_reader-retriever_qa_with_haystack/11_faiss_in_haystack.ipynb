{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS With Haystack\n",
    "\n",
    "FAISS is unfortunately **not** presently supported on Windows, so if you are on Windows then you will need to stick with Elasticsearch. If you have access to Linux or Mac then continue.\n",
    "\n",
    "We'll be using Haystack again, so fortunately setup is very straight-forward. We first import and initialize a FAISS document store using a very similiar logic to what we used before - but this time we will be storing the FAISS index locally.\n",
    "\n",
    "Storing the index locally means that we will need two files, a SQLite database, and the FAISS index. We create the FAISS index later, but we create the SQLite database on initialization.\n",
    "\n",
    "We will store both in the `models` directory, but adjust this to your own needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../models/faiss'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we include this path within a SQLite database location string in the following document store initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack import faiss\n",
    "from haystack.document_stores.faiss import FAISSDocumentStore\n",
    "\n",
    "# initialize FAISS\n",
    "document_store = FAISSDocumentStore(\n",
    "    faiss_index_factory_str='Flat',\n",
    "    sql_url=f'sqlite:///{path}/squad_dev.db',\n",
    "    return_embedding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our validation data from file, which we will be adding to the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../data/squad/dev.json', 'r') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data\n",
    "\n",
    "As we saw with Elasticsearch, our current FAISS index has been initialized but contains nothing. Now we need to populate the index with our *dev.json* data. \n",
    "\n",
    "This time, we'll be making use of the Haystack `Document` object. Which we import with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object prepares our data into the correct object format for our document stores - which in this case is FAISS.\n",
    "\n",
    "As before where we had a dictionary with two keys `'text'` and `'meta'`, the *Document* object provides two corresponding arguments, `text` and `meta`. So rather than using the format we used before which looked like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'text': '<document text here>',\n",
    "    'meta': {\n",
    "        'other': '<other info here>'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We will be using this *Document* object format instead:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    text='<document text here>',\n",
    "    meta={\n",
    "        'other': '<other info here>'\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Just like before, we will be feeding these *Document* objects into a list, which we will then feed into our FAISS `write_documents` method. Remember, our dataset contains duplicate contexts, so we must remove them first using `list(set(...))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of contexts\n",
    "contexts = [sample['context'] for sample in squad]\n",
    "\n",
    "# Remove duplicates\n",
    "contexts = list(set(contexts))\n",
    "\n",
    "# Create list of Document objects\n",
    "# squad_docs = [Document(text=sample) for sample in contexts]\n",
    "squad_docs = [Document(content=sample) for sample in contexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, because we're storing our FAISS index on file, we may find (if running this script more than once) that we first need to delete any documents that already exist in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_store.delete_all_documents()\n",
    "document_store.delete_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the data to the index just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da50ce1d2ce468e9aac6063d41115b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_store.write_documents(squad_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that our documents are indexed will depend on the embedding model being used by our retriever. So, we need to initialize our DPR model (the retriever), and then `update_embeddings` using this retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# from haystack.retriever.dense import DensePassageRetriever # Deprecated\n",
    "from haystack.nodes.retriever.dense import DensePassageRetriever\n",
    "\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model='facebook/dpr-question_encoder-single-nq-base',\n",
    "    passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',\n",
    "    use_gpu=True,\n",
    "    embed_title=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2251b4831b491c926c5cac18f2d7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/1204 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/1216 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fully prepared our document store, we can save it. We will save to the same location we saved our SQLite database, but this time we will be using the *.faiss* filetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.save(f'{path}/squad_dev.faiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our FAISS index is now saved to file! We'll go ahead and delete the `document_store` and `retriever`, and try reinitializing both using the data we've saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del document_store, retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we do now is apply the `load` method directly from `FAISSDocumentStore`, including both the FAISS index location, and SQLite database location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'index_factory'.\n  Possible C/C++ prototypes are:\n    faiss::index_factory(int,char const *,faiss::MetricType)\n    faiss::index_factory(int,char const *)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11591/1107830145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# document_store = FAISSDocumentStore.load(f'{path}/squad_dev.faiss', f'sqlite:///{path}/squad_dev.db')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdocument_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISSDocumentStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/squad_dev.faiss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'sqlite:///{path}/squad_dev.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36mwrapper_exportable_to_yaml\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Call the actuall __init__ function with all the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_exportable_to_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/document_stores/faiss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sql_url, vector_dim, embedding_dim, faiss_index_factory_str, faiss_index, return_embedding, index, similarity, embedding_field, progress_bar, duplicate_documents, faiss_index_path, faiss_config_path, isolation_level, n_links, ef_search, ef_construction, validate_index_sync)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaiss_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             self.faiss_indexes[index] = self._create_new_index(\n\u001b[0m\u001b[1;32m    148\u001b[0m                 \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mindex_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfaiss_index_factory_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/document_stores/faiss.py\u001b[0m in \u001b[0;36m_create_new_index\u001b[0;34m(self, embedding_dim, metric_type, index_factory, n_links, ef_search, ef_construction)\u001b[0m\n\u001b[1;32m    210\u001b[0m             )\n\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36mindex_factory\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   8691\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8692\u001b[0m     \"\"\"\n\u001b[0;32m-> 8693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8695\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindex_binary_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'index_factory'.\n  Possible C/C++ prototypes are:\n    faiss::index_factory(int,char const *,faiss::MetricType)\n    faiss::index_factory(int,char const *)\n"
     ]
    }
   ],
   "source": [
    "# document_store = FAISSDocumentStore.load(f'{path}/squad_dev.faiss', f'sqlite:///{path}/squad_dev.db')\n",
    "\n",
    "document_store = FAISSDocumentStore(f'{path}/squad_dev.faiss', f'sqlite:///{path}/squad_dev.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can re-initialize our retriever, using the same arguments as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model='facebook/dpr-question_encoder-single-nq-base',\n",
    "    passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',\n",
    "    use_gpu=True,\n",
    "    embed_title=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can begin retrieving relevant contexts to our questions using `retriever.retrieve`, which requires a single argument, `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.retrieve('What subject is most abstract?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we've extracted a few contexts stored within FAISS, that our DPR model believes answers our query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
