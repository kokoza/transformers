{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Similarity Search\n",
    "\n",
    "Finding nearest neighbor is done via brute force method, which makes it inefficient.\n",
    "\n",
    "See [Why FAISS Works](https://towardsdatascience.com/facebook-ai-similarity-search-7c564daee9eb) for more details.\n",
    "\n",
    "## FAISS Overview:\n",
    "\n",
    "0. Generate vector using Dense Passage Retriever\n",
    "1. Preprocess \n",
    "    * PCA\n",
    "    * L2 Normalization\n",
    "2. Inverted File Index (IVF)\n",
    "    * Finds similar vectors and clusters them (can specify the number of clusters to generate)\n",
    "    * Can compare our query to clusters rather than individual nodes; then compare our query to each vector in that cluster\n",
    "    * Approximate nearest neighbors search\n",
    "    * This is a non-exhaustive search -> not guaranteed to get the optimal solution\n",
    "3. Coarse Quantizer \n",
    "4. Fine Quantizer \n",
    "    * Flat method\n",
    "    * Product Quantization (PQ)\n",
    "    \n",
    "\n",
    "\n",
    "## What is FAISS\n",
    "\n",
    "**Facebook AI Similarity Search** (FAISS) is a more efficient implementation of *similarity searching* that can be used to efficiently scale to millions of samples without issue.\n",
    "\n",
    "Similarity search typically works by creating vectors from data, these vectors are represented as *points* in a highly-dimensional space (so like points in a 3D chart, but with 100+ dimensions).\n",
    "\n",
    "We can then introduce a new vector and search for similiar entries by comparing where it is within that highly dimensional space to all of the other vectors, often using Euclidean distance (L2 norm) or dot-product to measure similarity.\n",
    "\n",
    "At it's core, this is an approximate nearest neighbors (ANN) algorithm. However, FAISS performs multiple operations and transformations to speed up this process. These are split into four components:\n",
    "\n",
    "* Very efficient GPU usage\n",
    "\n",
    "* Preprocessing of vectors\n",
    "\n",
    "* Inverted File Quantizer (coarse quantizer)\n",
    "\n",
    "* Encoding Quantizer (fine quantizer)\n",
    "\n",
    "## Dense Passage Retriever (DPR)\n",
    "\n",
    "Now, when we searched through our Elasticsearch document store, we tested two different retrievers - TF-IDF and BM25. Both of these retrievers are referred to as *'sparse retrievers'*. Called sparse as they are using *sparse* vectors (vectors which are built of mostly zero/near-zero values). Alternatively, we have **dense retrievers**, which deal with *dense* vectors, which are packed full of relevant information (unlike sparse with their sparsely concentrated packets of information). We will be using a **Dense Passage Retriever (DPR)** alongside FAISS to retrieve relevant contexts.\n",
    "\n",
    "It is this combination of **Document Store (FAISS)** and **Retriever (DPR)** that we will be building in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS and Similary Search Meta Course\n",
    "\n",
    "* [FAISS and Similary Search Meta Course](https://www.youtube.com/playlist?list=PLIUOU7oqGTLhlWpTz4NnuT3FekouIVlqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
